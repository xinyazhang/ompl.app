/** 
\page benchmark How to benchmark planners

OMPL contains a Benchmark class that facilitates solving a motion planning
problem repeatedly with different planners, different samplers, or even 
differently configured versions of the same planning algorithm. Below, we
will describe how you can use this class.

- \ref benchmark_config
- \ref benchmark_code
- \ref benchmark_log
- \ref benchmark_sample_results

For more advanced benchmarks, please see <a href="http://plannerarena.org">plannerarena.org</a>.

\section benchmark_config Create a benchmark configuration file

OMPL.app contains a command line program called \c benchmark, that can read
a text based configuration file using an ini style format with key/value
pairs. This is the same format that can be read and saved with the OMPL.app
GUI. The GUI ignores the settings related to benchmarking. However, it is
often convenient to create an initial configuration with the GUI and add
the benchmark settings with a text editor. Currently the base functionality
of the \c benchmark program only applies to geometric planning
in SE(2) and SE(3), but can be extended by the user to other spaces.

There are a number of required parameters necessary to define the problem.  These
exist under the “[problem]” heading:
- <b>name</b>: An identifying name for the problem to be solved
- <b>robot</b>: The path to a mesh file describing the geometry of the robot
- <b>start.[x|y|z|theta]</b>: Values describing the start state of the robot
- <b>goal.[x|y|z|theta]</b>: Values describing the goal state of the robot

An optional <b>world</b> parameter specifying a mesh file for the environment can also
be specified, but is not required.  If unspecified, it is assumed that the robot operates
in an empty workspace.

Another optional parameter is the sampler to be used by the planner. The
following samplers are available:
- <b>sample=uniform</b> (this is the default if no sampler is specified)
- <b>sampler=gaussian</b>
- <b>sampler=obstacle_based</b>
- <b>sampler=max_clearance</b>
.

Parameters relating to benchmarking must be declared under the “[benchmark]”
heading:
- <b>time_limit</b>: The amount of time (seconds) for each plan computation
- <b>mem_limit</b>: The maximum amount of memory (MB) for each planner
- <b>run_count</b>: The number of times to repeat the experiment for each planner

The last required element to specify are the planners to benchmark.  These are specified
under the “[planner]” heading.  The following planners are valid for geometric benchmarking:
- <b>rrtconnect</b>
- <b>lazyrrt</b>
- <b>kpiece</b>
- <b>bkpiece</b>
- <b>lbkpiece</b>
- <b>est</b>
- <b>sbl</b>
- <b>prm</b>
- <b>rrt</b>

An example of a minimal SE(2) configuration comparing the rrt and est planners is given below:
\code
[problem]
name=my_benchmark_problem
robot=my_robot_mesh.dae
start.x=0.0
start.y=0.0
start.theta=0.0
goal.x=1.0
goal.y=1.0
goal.theta=0.0

[benchmark]
time_limit=10.0
mem_limit=1000.0
run_count = 3

[planner]
est=
rrt=
\endcode

Any parameter defined by these planners may also be configured for the benchmark.  For
example, the geometric::RRT planner defines two parameters, “range” and “goal_bias”, both
real valued.  The default values can be changed under the “planner” heading in the following
manner:
- <b>rrt.range</b>=50.0
- <b>rrt.goal_bias</b>=0.10

It is also convenient to specify the bounds of the workspace to plan in.  Without any
specification, OMPL.app assumes a tight bounding box around the start and goal states,
but depending on the environment this may not be a good assumption.  It is easy to
redefine the bounding box under the problem heading using the “volume” configuration:
- <b>volume.[min|max].[x|y|x]</b>

There are many other optional parameters that can be specified or changed.  The benchmark
executable takes advantage of the base::ParamSet class, and uses this functionality to
set any parameter defined in the file.  If a class exposes a parameter, chances are that
it is possible to tune it via the config file.  OMPL.app provides two example configuration
files inside of the benchmark directory, example.cfg and example_complex.cfg showing the
configuration of many of these optional parameters.

It is possible to create multiple instances of the same planner and configure each
differently. This code, for example, creates two instances of \c rrtconnect with 
different values for its range parameter:
\code
rrtconnect=
rrtconnect.range=100
rrtconnect=
rrtconnect.range=200
\endcode
Moreover, the problem settings can be changed between different planner instances.
Below, some of the problem settings are changed for the second instance of \c kpiece.
\code
kpiece=
kpiece=
# increase the size of the projection by a specific factor, in every dimension
problem.projection.cellsize_factor = 4.0
# specify a different sampler
problem.sampler=obstacle_based
\endcode

Finally, to execute the benchmark configuration file, simply run the \c benchmark executable
in the OMPL.app bin directory, and supply the path to the config file as the first argument.

\section benchmark_code Writing benchmarking code

Benchmarking a set of planners on a specified problem using the Benchmark class in your own
code is a simple task in OMPL. The steps involved are as follows:
- Configure the benchmark problem using ompl::geometric::SimpleSetup or ompl::control::SimpleSetup
- Create a ompl::Benchmark object that takes the problem as input
- Add one or more planners to the benchmark
- Optionally add events to be called before and/or after the execution of a planner
- Run the benchmark problem a specified number of times, subject to specified time and memory limits
.

The following code snippet shows you how to do this. We will start with some initial code that you have probably already used:
\code
#include "ompl/tools/benchmark/Benchmark.h"

// A function that matches the ompl::base::PlannerAllocator type.
// It will be used later to allocate an instance of EST
ompl::base::PlannerPtr myConfiguredPlanner(const ompl::base::SpaceInformationPtr &si)
{
    geometric::EST *est = new ompl::geometric::EST(si);
    est->setRange(100.0);
    return ompl::base::PlannerPtr(est);
}

// Create a state space for the space we are planning in
ompl::geometric::SimpleSetup ss(space);

// Configure the problem to solve: set start state(s)
// and goal representation
// Everything must be set up to the point ss.solve()
// can be called. Setting up a planner is not needed.
\endcode

Benchmarking code starts here:
\code
// First we create a benchmark class:
ompl::Benchmark b(ss, "my experiment");

// We add the planners to evaluate.
b.addPlanner(base::PlannerPtr(new geometric::KPIECE1(ss.getSpaceInformation())));
b.addPlanner(base::PlannerPtr(new geometric::RRT(ss.getSpaceInformation())));
b.addPlanner(base::PlannerPtr(new geometric::SBL(ss.getSpaceInformation())));
b.addPlanner(base::PlannerPtr(new geometric::LBKPIECE1(ss.getSpaceInformation())));
// etc

// For planners that we want to configure in specific ways,
// the ompl::base::PlannerAllocator should be used:
b.addPlannerAllocator(boost::bind(&myConfiguredPlanner, _1));
// etc.

// Now we can benchmark: 5 second time limit for each plan computation,
// 100 MB maximum memory usage per plan computation, 50 runs for each planner
// and true means that a text-mode progress bar should be displayed while
// computation is running.
ompl::Benchmark::Request req;
req.maxTime = 5.0;
req.maxMem = 100.0;
req.runCount = 50;
req.displayProgress = true;
b.benchmark(req);

// This will generate a file of the form ompl_host_time.log
b.saveResultsToFile();
\endcode
Adding callbacks for before and after the execution of a run is also possible:
\code
// Assume these functions are defined
void optionalPreRunEvent(const base::PlannerPtr &planner)
{
   // do whatever configuration we want to the planner,
   // including changing of problem definition (input states)
   // via planner->getProblemDefinition()
}

void optionalPostRunEvent(const base::PlannerPtr &planner, Benchmark::RunProperties &run)
{
   // do any cleanup, or set values for upcoming run (or upcoming call to the pre-run event).

   // adding elements to the set of collected run properties is also possible;
   // (the added data will be recorded in the log file)

   run["some extra property name INTEGER"] = "some value";
   // The format of added data is string key, string value pairs,
   // with the convention that the last word in string key is one of
   // REAL, INTEGER, BOOLEAN, STRING. (this will be the type of the field
   // when the log file is processed and saved as a database).
   // The values are always converted to string.
}

// After the Benchmark class is defined, the events can be optionally registered:
b.setPreRunEvent(boost::bind(&optionalPreRunEvent, _1));
b.setPostRunEvent(boost::bind(&optionalPostRunEvent, _1, _2));
\endcode

\section benchmark_log Processing the benchmarking log file

Once the C++ code computing the results has been executed, a log
file is generated. This contains information about the settings of
the planners, the parameters of the problem tested on, etc.
To visualize this information, we provide a script that parses the log files:
\code
scripts/benchmark_statistics.py logfile.log -d mydatabase.db
\endcode
This will generate a SQLite database containing the parsed data. If no
database name is specified, the named is assumed to be benchmark.db
Once this database is generated, we can construct plots:
\code
scripts/benchmark_statistics.py -d mydatabase.db -p boxplot.pdf
\endcode
This will generate a series of plots, one for each of the attributes described below, showing the results for each planner. \ref benchmark_sample_results "Below" we have included some sample benchmark results.

If you would like to process the data in different ways, you can generate
a dump file that you can load in a MySQL database:
\code
scripts/benchmark_statistics.py -d mydatabase.db -m mydump.sql
\endcode
The database will contain 2+<em>k</em> tables:
- \e planners is a table that contains planner configurations
- \e experiments is a table that contains details about conducted experiments
- <em>k</em> tables named \e planner_<name>, one for each planner, containing measurements

For more details on how to use the benchmark script, see:
\code
scripts/benchmark_statistics.py --help
\endcode

Collected benchmark data for each experiment:
- <strong>name:</strong> name of experiment (optional)
- <strong>totaltime:</strong> the total duration for conducting the experiment (seconds)
- <strong>timelimit:</strong> the maximum time allowed for every planner execution (seconds)
- <strong>memorylimit:</strong> the maximum memory allowed for every planner execution (MB)
- <strong>hostname:</strong> the name of the host on which the experiment was run
- <strong>date:</strong> the date and time when the experiment was started

Collected benchmark data for each planner execution:
- <strong>time:</strong> (real) the amount of time spent planning, in seconds
- <strong>memory:</strong> (real) the amount of memory spent planning, in MB. Note: this may be inaccurate since memory is often freed in a lazy fashion
- <strong>solved:</strong> (boolean) flag indicating whether the planner found a solution. Note: the solution can be approximate
- <strong>approximate solution:</strong> (boolean) flag indicating whether the found solution is approximate (does not reach the goal, but moves towards it)
- <strong>solution difference:</strong> (real) if the solution is approximate, this is the distance from the end-point of the found approximate solution to the actual goal
- <strong>solution length:</strong> (real) the length of the found solution
- <strong>solution smoothness:</strong> (real) the smoothness of the found solution (the closer to 0, the smoother the path is)
- <strong>solution clearance:</strong> (real) the clearance of the found solution (the higher the value, the larger the distance to invalid regions)
- <strong>solution segments:</strong> (integer) the number of segments on the solution path
- <strong>correct solution:</strong> (boolean) flag indicating whether the found solution is correct (a separate check is conducted). This should always be true.
- <strong>correct solution strict:</strong> (boolean) flag indicating whether the found solution is correct when checked at a finer resolution than the planner used when validating motion segments. If this is sometimes false it means that the used state validation resolution is too high (only applies when using ompl::base::DiscreteMotionValidator).
- <strong>simplification time:</strong> (real) the time spend simplifying the solution path, in seconds
- <strong>simplified solution length:</strong> (real) the length of the found solution after simplification
- <strong>simplified solution smoothness:</strong> (real) the smoothness of the found solution after simplification (the closer to 0, the smoother the path is)
- <strong>simplified solution clearance:</strong> (real) the clearance of the found solution after simplification (the higher the value, the larger the distance to invalid regions)
- <strong>simplified solution segments:</strong> (integer) the number of segments on solution path after simplification
- <strong>simplified correct solution:</strong> (boolean) flag indicating whether the found solution is correct after simplification. This should always be true.
- <strong>simplified correct solution strict:</strong> (boolean) flag indicating whether the found solution is correct after simplification, when checked at a finer resolution.
- <strong>graph states:</strong> (integer) the number of states in the constructed graph
- <strong>graph motions:</strong> (integer) the number of edges (motions) in the constructed graph
- <strong>valid segment fraction:</strong> (real) the fraction of segments that turned out to be valid (using ompl::base::MotionValidator) out of all the segments that were checked for validity
- more planner-specific properties

\section benchmark_sample_results Sample benchmark results

Below are sample results for running benchmarks for two example problems: the “cubicles” environment and the “Twistycool” environment. The complete benchmarking program (SE3RigidBodyPlanningBenchmark.cpp), the environment and robot files are included with OMPL.app, so you can rerun the exact same benchmarks on your own machine. See the \ref gallery_omplapp "gallery" for visualizations of sample solutions to both problems. The results below were run on a recent model Apple MacBook Pro (2.66 GHz Intel Core i7, 8GB of RAM). It is important to note that none of the planner parameters were tuned; all benchmarks were run with default settings. From these results one cannot draw any firm conclusions about which planner is “better” than some other planner.

These are the PDF files with plots as generated by the benchmark_statistics.py script:
- <a href="../images/cubicles.pdf">The “cubicles” problem with default settings</a>
- <a href="../images/Twistycool.pdf">The “Twistycool” problem with default settings</a>
.
The plots show comparisons between ompl::geometric::RRTConnect, ompl::geometric::RRT, ompl::geometric::BKPIECE1, ompl::geometric::LBKPIECE1, ompl::geometric::KPIECE1, ompl::geometric::SBL, ompl::geometric::EST, and ompl::geometric::PRM.
Each planner is run 500 times with a 10 second time limit for the cubicles problem for each sampling strategy, while for the Twistycool problem each planner is run 50 times with a 60 second time limit.

For integer and real-valued measurements the script will compute <a href="http://en.wikipedia.org/wiki/Box_plot">box plots</a>. For example, here is the plot for the real-valued attribute <strong>time</strong> for the cubicles environment:

\htmlonly<div class="row"><img src="../images/cubicles_time.png" class="span8 offset1"></div>\endhtmlonly

For boolean measurements the script will create bar charts with the percentage of \b true values. For example, here is the plot for the boolean attribute <strong>solved</strong> for the Twistycool environment, a much harder problem:

\htmlonly<div class="row"><img src="../images/Twistycool_solved.png" class="span8 offset1"></div>\endhtmlonly

Whenever measurements are not always available for a particular attribute, the columns for each planner are labeled with the number of runs for which no data was available. For instance, the boolean attribute <strong>correct solution</strong> is not set if a solution is not found.
*/
